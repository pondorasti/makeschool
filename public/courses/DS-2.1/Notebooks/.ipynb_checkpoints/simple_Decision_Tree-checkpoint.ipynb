{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/slide_1.png\" width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Images/slide_2.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "- Decision Trees are considered one of the most mature, traditional, algorithms in predictive analytics\n",
    "- They are typically used to solve classification problems through visual and explicit representations of decisions and decision making.\n",
    "- Think of them like a map where you follow each path according to your decision, and each path leads to a new choice to make until you reach the end.\n",
    "- They mimic the way you probably make decisions in your daily life:\n",
    "\n",
    "![decision_tree](./Images/dec_tree.png)\n",
    "\n",
    "### Terminology\n",
    "\n",
    "- **Root:** Our starting point for the tree. Note that a decision tree is drawn upside down since its root is at the top\n",
    "    - `Alone Or With Friends` is the root in the above example\n",
    "- **Branch:** Also known as an _edge_, these lead from condition to condition, down to the results\n",
    "    - `Sunny` or `Rainy` are branches in the above example\n",
    "- **Condition:** Also known as an _internal node_, this is the choice that needs to be made in order to figure out which branch to take.\n",
    "    - `Weather Outside?` is our condiition in the above example\n",
    "- **Leaf:** Also known as a _decision_, these are the final results that signify the classification of the data. There are no branches coming out of a leaf, only going in to it.\n",
    "    - `video games`, `soccer` and `movies` are all examples of a leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question to the class: Why and when do we need Decision Trees?\n",
    "\n",
    "### Shout out or type your answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our answers:\n",
    "\n",
    "- When features are Categorical\n",
    "    - When we can classify data into known groups\n",
    "- When we want to model a set of sequential, hierarchical decisions that lead to some final result. This result is the known group that the data point would be categorized into\n",
    "- When we need to explain the reason for a particular decision\n",
    "- Example use cases:\n",
    "    - Sales and marketing departments might need a complete description of rules that influence the acquisition of a customer before they start their campaign activities\n",
    "    - Product planning (do we build this product or not?)\n",
    "    - Determining someone is a good or bad level of risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The root and the leafs for Decision Tree are obtained based on: \n",
    "\n",
    "- Conditional Probability\n",
    "\n",
    "- Entropy\n",
    "\n",
    "- Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lens Dataset\n",
    "\n",
    "Let's review the Attribute Information that we know:\n",
    "\n",
    "We have 3 Classes (leaves/results)\n",
    "\n",
    "1. the patient should be fitted with _hard_ contact lenses,\n",
    "1. the patient should be fitted with _soft_ contact lenses,\n",
    "1. the patient should _not be fitted_ with contact lenses.\n",
    " \n",
    "The dataset has 4 Features (conditions):\n",
    "\n",
    "1. age of the patient: (1) young, (2) pre-presbyopic, (3) presbyopic\n",
    "1. spectacle prescription:  (1) myope, (2) hypermetrope\n",
    "1. astigmatic:     (1) no, (2) yes\n",
    "1. tear production rate:  (1) reduced, (2) normal\n",
    "\n",
    "Here is the data used for the Decision Tree:\n",
    "\n",
    "<img src=\"Images/lens_dataset.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lens Decision Tree Visualized\n",
    "\n",
    "This is ultimately what we want to build using the above dataset\n",
    "\n",
    "<img src=\"Images/lens_DT.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees are based on Entropy\n",
    "\n",
    "## Activity: Calculate the entropy for a coin\n",
    "\n",
    "**Entropy** shows the uncertainy of a random variable. The higher the entropy value, the more unncertain we are. Entropy is displayed as $H(X)$, where $X$ is a random variable\n",
    "\n",
    "The Entropy formula is the summation of probabilities multiplied by the log of probabilities:\n",
    "\n",
    "<!-- `Entropy(Coin) = sum(-p(outcome)xlog2p(outcome) for outcome in [H, T])` -->\n",
    "\n",
    "### Entropy of coin\n",
    "\n",
    "Given `p` stands for \"probability of\", \n",
    "\n",
    "for `outcome` in `[H,T]`:\n",
    "\n",
    "- $H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "### Entropy of a fair coin\n",
    "\n",
    "<!-- `Entropy(Coin) = sum(-p(outcome)xlog2p(outcome) for p(outcome) in [p(H)=1/2, p(T)=1/2])` -->\n",
    "\n",
    "for `p(outcome)` in `[p(H)=0.5, p(T)=0.5])`:\n",
    "\n",
    "- $H(Coin) = \\sum -p(outcome) * log_2(p(outcome)$\n",
    "\n",
    "**Do the following in pairs:**\n",
    "\n",
    "- Create a function `entropy` that takes an array of probabilities as input, and returns the entropy using the formula above\n",
    "    - numpy's `array`, `log2`, and `sum` functions should be useful here\n",
    "- show that the fair coin has the largest entropy (uncertainty) by trying different values for  the probability of heads and tails\n",
    "    - i.e. show that a fair coin `[.5, .5]` has a larger entropy than a coin with `[.9, .1]` probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "print(entropy(p))\n",
    "\n",
    "p = [.9, .1]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Change p (probability of head and tail) and plot the entropy for different values of p\n",
    "\n",
    "<img src=\"Images/coin_entropy.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    " The fair coin has the highest entropy which means a fair coin has the highest uncertain result when toss a coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How We'll Use Decision Trees Today\n",
    "\n",
    "- You’ll see if a decision tree can give you any insight as to how the eye doctor prescribes contact lenses\n",
    "\n",
    "- You can predict the type of lenses people will use and understand the underlying processes with a decision tree\n",
    "\n",
    "- Predict if a tennis player will play outside based on weather conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Review on Conditional Probability\n",
    "\n",
    "We'll be using conditional probability to solve the following activities. Before we do so, let's take 10 minutes to [review conditional probability from DS 1.1](https://github.com/Make-School-Courses/DS-1.1-Data-Analysis/blob/master/Notebooks/Applied_Probability.ipynb)\n",
    "\n",
    "We'll even see the same data set we're about to build a decision tree for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's build a Decision Tree for the Tennis Data\n",
    "\n",
    "The following table shows us the decision making factors used to play tennis outside based on 14 days of data for different weather conditions\n",
    "\n",
    "<img src=\"Images/dst_1.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity: Obtain the following quantitites:\n",
    "\n",
    "**In groups of 3:** Using the [tennis dataset](./Datasets/tennis.txt), obtain the following quantities:\n",
    "\n",
    "### Entropy for PlayTennis:\n",
    "\n",
    "Obtain the entropy of the`PlayTennis` (Leaf/Decision) column. \n",
    "\n",
    "### Entropy for PlayTennis conditioned on Weak Wind factor\n",
    "\n",
    "Obtain the entropy of conditional probability `p(PlayTennis | Wind = Weak) = [2/8, 6/8]`\n",
    "\n",
    "### Entropy for PlayTennis conditioned on Strong Wind factor\n",
    "\n",
    "Obtain the entropy of conditional probability `p(PlayTennis | Wind = Strong) = [3/6, 3/6]`\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- p = [9/14, 5/14] which represents the probability that a player plays tennis (9/14 days) or not (5/14 days)\n",
    "- Remember your Entropy function from earlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solutions\n",
    "\n",
    "`Entropy(Decision) = – (9/14) . log2(9/14) – (5/14) . log2(5/14) = 0.940`\n",
    "\n",
    "<img src=\"Images/weak_wind_decision.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "<img src=\"Images/strong_wind_decision.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "**Information Gain** measures how much information a feature gives us about the decision (class). This is the main measurement used by a Decision Tree algorithm to construct a Decision Tree!\n",
    "\n",
    "- Decision Trees will always try to maximize information gain\n",
    "- The higher the information gain a feature has, the more likely it is to be tested first\n",
    "    - the feature with the highest information gain will be the first feature in the decision tree, and its branches will lead to the other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obtain the Information Gain Between PlayTennis (Decision) and Wind\n",
    "\n",
    "- What is the probability that wind be weak? \n",
    "\n",
    "**Hint:** Count how many instannces of weak and strong winds we have divided by how many sample we have.\n",
    "\n",
    "`p(Wind = Weak) = 8/ 14`\n",
    "\n",
    "`p(Wind = Strong) = 6/ 14`\n",
    "\n",
    "Below are the formulas for finding Information Gain $I(X; Y)$ for a given decision $X$ and feature $Y$, and the Entropy for a decision given a feature\n",
    "\n",
    "<img src=\"Images/Information_Gain.png\" width=\"=500\" height=\"500\">\n",
    "\n",
    "<img src=\"Images/Conditional_Entropy.png\" width=\"=500\" height=\"500\">\n",
    "\n",
    "Given `p` stands for \"probability of\", \n",
    "\n",
    "for `Wind = {Weak, Strong}`:\n",
    "\n",
    "- $I(Decision; Wind) = H(Decision) - \\sum p(Wind) * Entropy(Decision | Wind)$\n",
    "\n",
    "We can break this down further:\n",
    "\n",
    "$H(Decision) - \\sum p(Wind) * Entropy(Decision | Wind)$\n",
    "\n",
    "$=$ \n",
    "\n",
    "$H(Decision) - (p(Wind = Weak) * H(Decision | Wind = Weak) + p(Wind = Strong) * H(Decision | Wind = Strong)) = 0.048$\n",
    "\n",
    "<!-- `Information Gain(Decision, Wind) = Entropy(Decision) - sum(p(Wind) * Entropy(Decision | Wind )) for Wind = {Weak, Strong}`-->\n",
    "\n",
    "<!--`Entropy(Decision) - (p(Wind = Weak)Entropy(Decision | Wind = Weak ) + p(Wind = Strong)Entropy(Decision | Wind = Strong )) = 0.048`-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other factors on Decision column\n",
    "\n",
    "We have applied similar calculation on the other features (columns)\n",
    "\n",
    "1 - Information Gain(Decision, Wind) = 0.048\n",
    "\n",
    "2 - Information Gain(Decision, Outlook) = 0.246\n",
    "\n",
    "3 - Information Gain(Decision, Temperature) = 0.029\n",
    "\n",
    "4 - Information Gain(Decision, Humidity) = 0.151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see Outlook and Decision have the highest Gain, so Outlook will be the root for the Decision Tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we keep continuing the calculation of Information Gain between nodes (features), we can build the tree based on the highest Information Gains from feature to feature\n",
    "\n",
    "Example: Information Gain (Outlook, Wind), Information Gain (Outlook, Temperature), Information Gain (Outlook, Humidity), and then finding the information gain after that, and so on and so forth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build the decision tree with sklearn for tennis dataset\n",
    "\n",
    "#### For Decision Tree Visualization in Python:\n",
    "\n",
    "Packages that are needed are below. Note that the multiple installs for graphviz are to ensure the executables install correctly to avoid [this error](https://stackoverflow.com/questions/28312534/graphvizs-executables-are-not-found-python-3-4):\n",
    "\n",
    "`conda install -c anaconda graphviz`\n",
    "\n",
    "`brew install graphviz`\n",
    "\n",
    "`conda install -c anaconda pydot`\n",
    "\n",
    "`conda install -c conda-forge pydotplus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook  Temp Humidity    Wind Play\n",
      "1      Sunny   Hot     High    Weak   No\n",
      "2      Sunny   Hot     High  Strong   No\n",
      "3   Overcast   Hot     High    Weak  Yes\n",
      "4       Rain  Mild     High    Weak  Yes\n",
      "5       Rain  Cool   Normal    Weak  Yes\n",
      "6       Rain  Cool   Normal  Strong   No\n",
      "7   Overcast  Cool   Normal  Strong  Yes\n",
      "8      Sunny  Mild     High    Weak   No\n",
      "9      Sunny  Cool   Normal    Weak  Yes\n",
      "10      Rain  Mild   Normal    Weak  Yes\n",
      "11     Sunny  Mild   Normal  Strong  Yes\n",
      "12  Overcast  Mild     High  Strong  Yes\n",
      "13  Overcast   Hot   Normal    Weak  Yes\n",
      "14      Rain  Mild     High  Strong   No\n",
      "    Outlook  Temp  Humidity  Wind  Play\n",
      "1         2     1         0     1     0\n",
      "2         2     1         0     0     0\n",
      "3         0     1         0     1     1\n",
      "4         1     2         0     1     1\n",
      "5         1     0         1     1     1\n",
      "6         1     0         1     0     0\n",
      "7         0     0         1     0     1\n",
      "8         2     2         0     1     0\n",
      "9         2     0         1     1     1\n",
      "10        1     2         1     1     1\n",
      "11        2     2         1     0     1\n",
      "12        0     2         0     0     1\n",
      "13        0     1         1     1     1\n",
      "14        1     2         0     0     0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# read in the tennis data, need the extra parameters since it's a txt file\n",
    "data = pd.read_csv('./Datasets/tennis.txt', delimiter=\"\\t\", header=None, names=['Outlook', 'Temp', 'Humidity', 'Wind', 'Play'])\n",
    "print(data)\n",
    "\n",
    "# encode the data so we can use it with our decision tree,\n",
    "# by converting categories into numbers\n",
    "data_encoded = data.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "print(data_encoded)\n",
    "\n",
    "# create our decision tree classifier with entropy\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "\n",
    "# one_hot_data = pd.get_dummies(data[['a', 'b', 'c', 'd']], drop_first=True)\n",
    "# print(one_hot_data)\n",
    "\n",
    "# provide our feature array and target array (1-item),\n",
    "# and train the model using a decision tree\n",
    "clf.fit(data_encoded[['Outlook', 'Temp', 'Humidity', 'Wind']], data_encoded['Play'])\n",
    "\n",
    "# export our decision tree into data that can be plotted\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=['Outlook', 'Temp.', 'Humidity', 'Wind'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png('tennis_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Information Gain between Feature and Target\n",
    "\n",
    "**In groups of 3:** Write a function that takes a Tennis pandas dataframe, one feature (for example Wind) and target column (Decision) as input, then returns the information gain between the feature and the target. This should work for any feature (Outlook, Temp, Humidity, Wind) and the target (Play) for the given dataset.\n",
    "\n",
    "Use the hints and starter code given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook  Temp Humidity    Wind Play\n",
      "1      Sunny   Hot     High    Weak   No\n",
      "2      Sunny   Hot     High  Strong   No\n",
      "3   Overcast   Hot     High    Weak  Yes\n",
      "4       Rain  Mild     High    Weak  Yes\n",
      "5       Rain  Cool   Normal    Weak  Yes\n",
      "6       Rain  Cool   Normal  Strong   No\n",
      "7   Overcast  Cool   Normal  Strong  Yes\n",
      "8      Sunny  Mild     High    Weak   No\n",
      "9      Sunny  Cool   Normal    Weak  Yes\n",
      "10      Rain  Mild   Normal    Weak  Yes\n",
      "11     Sunny  Mild   Normal  Strong  Yes\n",
      "12  Overcast  Mild     High  Strong  Yes\n",
      "13  Overcast   Hot   Normal    Weak  Yes\n",
      "14      Rain  Mild     High  Strong   No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tennis.txt', delimiter=\"\\t\", header=None, names=['Outlook', 'Temp', 'Humidity', 'Wind', 'Play'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: helper function for entopy\n",
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-i*np.log2(i) for i in p]).sum()\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'No': 0.25, 'Yes': 0.75}\n",
      "{'No': 0.5, 'Yes': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# hint: helper function that takes a dataset (df) and one of its features (c1),\n",
    "# decision (c2), and condition of the feature (condition) as input, and outputs\n",
    "# the condiitional probability\n",
    "def conditional_prob(df, c1, c2, condition):\n",
    "    df_new = df[df[c1] == condition][c2]\n",
    "    s = df_new.unique()\n",
    "    population_size = len(df_new)\n",
    "    pr = {}\n",
    "    for i in s:\n",
    "        pr[i] = len(df[(df[c1] == condition) & (df[c2]== i)]) / population_size\n",
    "    \n",
    "    return pr\n",
    "\n",
    "# what are the probabilities of Play  given Wind is Weak?\n",
    "print(conditional_prob(data,'Wind', 'Play', 'Weak'))\n",
    "\n",
    "# what are the probabilities of Play given Wind is Strong?\n",
    "print(conditional_prob(data, 'Wind', 'Play', 'Strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: dataset (df), a feature from the dataset (feature), and the target (decision)\n",
    "# returns: information gain between feature and decision\n",
    "def info_gain(df, feature, decision):\n",
    "    # obtain the entropy of the decision\n",
    "    dict_decision = dict(df[decision].value_counts())\n",
    "    prob_decision = [q for (p,q) in dict_decision.items()]/sum(dict_decision.values())\n",
    "    entropy_decision = entropy(prob_decision)\n",
    "#     print(entropy_decision)\n",
    "    \n",
    "    # obtain the probabilities of the feature\n",
    "    # example: for Wind, obtain the probabilities of Strong and Weak\n",
    "    dict_feature = dict(df[feature].value_counts())\n",
    "    dict_prob_feature = {}\n",
    "    for (p,q) in dict_feature.items():\n",
    "        dict_prob_feature[p] = q/sum(dict_feature.values())\n",
    "#     print(dict_prob_feature)\n",
    "    \n",
    "    # obtain the probability of the decision,\n",
    "    # for all possible values of the feature (conditions)\n",
    "    conditions = df[feature].unique()\n",
    "    dict_ = {}\n",
    "    for condition in conditions:\n",
    "        dict_[condition] = conditional_prob(df, feature, decision, condition)\n",
    "#     print(dict_)\n",
    "    \n",
    "    # Given the above metrics, calculate the information gain\n",
    "    # between the feature and the decision using the formula we learned\n",
    "    S = 0\n",
    "    for (i,j) in dict_.items():\n",
    "#         print(i,j)\n",
    "        prob_condition = list(dict_[i].values())\n",
    "#         print(entropy_condition)\n",
    "        S = S + dict_prob_feature[i]*entropy(prob_condition)\n",
    "#         print(dict_prob_feature[i]*entropy(entropy_condition))\n",
    "    print(entropy_decision - S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04812703040826949\n",
      "0.15183550136234159\n",
      "0.02922256565895487\n",
      "0.24674981977443933\n"
     ]
    }
   ],
   "source": [
    "info_gain(data, 'Wind', 'Play')\n",
    "info_gain(data, 'Humidity', 'Play')\n",
    "info_gain(data, 'Temp', 'Play')\n",
    "info_gain(data, 'Outlook', 'Play')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: We can show the information gain between any feature with itself is equal to its entropy: \n",
    "\n",
    "- $I(X, X) = H(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5774062828523454\n",
      "1.5774062828523454\n",
      "1.5566567074628228\n",
      "1.5566567074628228\n",
      "1.0\n",
      "1.0\n",
      "0.9852281360342515\n",
      "0.9852281360342515\n",
      "0.9402859586706311\n",
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "for i in ['Outlook', 'Temp', 'Humidity', 'Wind', 'Play']:\n",
    "    p = [m/sum(data[i].value_counts().to_dict().values()) for m in list(data[i].value_counts().to_dict().values())]\n",
    "    print(entropy(p))\n",
    "    info_gain(data, i, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences\n",
    "\n",
    "- Decision Trees provide an effective method of Decision Making \n",
    "\n",
    "- The best characteristic of using trees for analytics -> easy to interpret and explain to executives\n",
    "\n",
    "- The mechanism behind Decision Trees is maximizing information gains while have different options "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources:\n",
    "\n",
    "- https://sefiks.com/2017/11/20/a-step-by-step-id3-decision-tree-example/\n",
    "\n",
    "- https://heartbeat.fritz.ai/introduction-to-decision-tree-learning-cd604f85e236\n",
    "\n",
    "- https://journals.plos.org/plosone/article/figure/image?id=10.1371/journal.pone.0161696.g002&size=inline\n",
    "\n",
    "- https://medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01\n",
    "\n",
    "- https://pdfs.semanticscholar.org/26e0/a38b6a81802d9d3c7fdd430befda7ea6bf11.pdf\n",
    "\n",
    "DT Visualization:\n",
    "\n",
    "- https://explained.ai/decision-tree-viz/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mild': 6, 'Cool': 4, 'Hot': 4}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Temp'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 4]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['Temp'].value_counts().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data['Temp'].value_counts().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5566567074628228\n"
     ]
    }
   ],
   "source": [
    "p = [m/sum(data['Temp'].value_counts().to_dict().values()) for m in list(data['Temp'].value_counts().to_dict().values())]\n",
    "\n",
    "print(entropy(p))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
