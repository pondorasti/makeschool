{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Neural Network?\n",
    "\n",
    "It is a computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Artificial Neural Networks\n",
    "\n",
    "A large number of very simple processing neuron-like processing elements\n",
    "\n",
    "A large number of weighted connections between the elements\n",
    "\n",
    "Distributed representation of knowledge over the connections\n",
    "\n",
    "Knowledge is acquired by network through a learning process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is perceptron?\n",
    "\n",
    "A perceptron can be understood as anything that takes multiple inputs and produces one output\n",
    "\n",
    "<img src=\"perceptron.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron (MLP)\n",
    "\n",
    "MLP is the stack of perceptrons \n",
    "\n",
    "<img src=\"MLP.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "In this image, the yellow nodes are inputs, the blue nodes (at each vertical) are hidden layers and the orange ones are output of the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and backward propagation\n",
    "\n",
    "NN takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as “***Forward Propagation***“\n",
    "\n",
    "Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function. \n",
    "\n",
    "We try to obtain the weight of neurons such that the NN total error (our cost function) being minimized. This process is known as “***Backward Propagation***“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Implementing NN using Numpy\n",
    "\n",
    "Assume, we want to build and train (obtain the weigths) of a MLP such that for the given input:\n",
    "\n",
    "`X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])`\n",
    "\n",
    "gives us this desire ouput:\n",
    "\n",
    "`y=np.array([[1],[1],[0]])`\n",
    "\n",
    "Also, assume we have only one hidden layer with three neurons and activation function for each perceptron is sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NN_block_diag.PNG\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    #h is hiddenlayer_activations\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    # y_t - y_p is D\n",
    "    D = y-output\n",
    "    # y_p*(1 - y_p) is slope_output_layer\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "#     slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "#     Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "#     d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "#     bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "#     wh += X.T.dot(d_hiddenlayer) *lr\n",
    "#     bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we update the weight to minimize the error?\n",
    "\n",
    "First we should define the cost function. For our example here the MSE is our cost function:\n",
    "\n",
    "$E= \\frac{1}{2} ({\\bf y}_t - {\\bf y}_p)^T ({\\bf y}_t - {\\bf y}_p)$ \n",
    "\n",
    "We update the weight (${\\bf W}_i$ and ${\\bf W}_h$) such that the error, $E$, being minimized. The most popular algorithm is Gradient Descent:  \n",
    "\n",
    "${\\bf W}_h = {\\bf W}_h - \\eta {\\partial E}/{\\partial {\\bf W}_h} $\n",
    "\n",
    "For our above example we can show that:\n",
    "\n",
    "${\\partial E}/{\\partial {\\bf W}_h} =  ({\\bf y}_t - {\\bf y}_p) {\\bf y}_p (1 - {\\bf y}_p)\\bf {h}$ \n",
    "\n",
    "where ${\\bf h} = \\sigma({\\bf W}_i {\\bf x}_i + {\\bf b}_i)$ \n",
    "\n",
    "In above code:\n",
    "\n",
    "$D = {\\bf y}_t - {\\bf y}_p$\n",
    "\n",
    "${\\bf y}_p (1 - {\\bf y}_p)$ = `slope_hidden_layer`\n",
    "\n",
    "$\\bf {h}$ = `hiddenlayer_activations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode of the above Neural Network \n",
    "\n",
    "<img src=\"pseudocode_NN.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is not good with this approache?\n",
    "\n",
    "- We should compute the derivate of cost function w.r.t weights and biases (of hidden and output layer) by hand "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
