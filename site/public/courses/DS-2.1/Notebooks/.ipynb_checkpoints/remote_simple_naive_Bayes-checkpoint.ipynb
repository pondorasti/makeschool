{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of today's class, you should be able to...\n",
    "\n",
    "- Review Bayes'formula for conditional probability \n",
    "\n",
    "- Apply Bayes' rule for text classification\n",
    "\n",
    "- Write a Python function for text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification is the process of attaching labels to bodies of text, e.g., tax document, medical form, etc. based on the content of the text itself.\n",
    "\n",
    "Think of your spam folder in your email. How does your email provider know that a particular message is spam or “ham” (not spam)?\n",
    "\n",
    "### Question: How do _you_ tell if an email is spam or ham? What are the signs?\n",
    "\n",
    "#### Followup: How does your process differ from a text classifier's?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of conditional probability and its application on Text\n",
    "\n",
    "- Assume this small dataset is given:\n",
    "\n",
    "<img src=\"Images/spam_ham_data_set.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What is the probability that an email is spam? What is the probability that an email is ham?\n",
    "\n",
    "- $P(spam) = ?$\n",
    "\n",
    "- $P(ham) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Create spam and ham dictionary\n",
    "\n",
    "- Create two dictionaries for spam and ham where keys are unique words and values are the frequency of each word\n",
    "    - Example: if the word \"password\" shows up 4 times in the text, then in the dictionary, the key would be \"password\" and the value would be 4\n",
    "- Create the dictionaries programatically using `for` loops\n",
    "- Use the below text to create your dictionaries:\n",
    "    - `spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']`\n",
    "    - `ham_text= ['Send us your review', 'review your password']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Dictionary:\n",
      "{'send': 3, 'us': 3, 'your': 3, 'password': 2, 'review': 1, 'account': 1}\n",
      "\n",
      "\n",
      "Ham Dictionary:\n",
      "{'send': 1, 'us': 1, 'your': 2, 'review': 2, 'password': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_text= ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "\n",
    "ham_text= ['Send us your review', 'review your password']\n",
    "\n",
    "spam = {}\n",
    "for i in spam_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in spam:\n",
    "            spam[j] = 1\n",
    "        else:\n",
    "            spam[j] += 1\n",
    "\n",
    "print(\"Spam Dictionary:\")            \n",
    "print(spam)\n",
    "print(\"\\n\")\n",
    "\n",
    "ham = {}\n",
    "for i in ham_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in ham:\n",
    "            ham[j] = 1\n",
    "        else:\n",
    "            ham[j] += 1\n",
    "\n",
    "print(\"Ham Dictionary:\")\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Given our dictionaries from the last activity, if we know an email is spam, what is the probability that the word \"password\" is in the email? \n",
    "\n",
    "What is the frequency of \"password\" in a spam email?\n",
    "\n",
    "- Answer:\n",
    "\n",
    " $P(password \\mid spam) = 2/(3+3+3+2+1+1) = 2/13 \\approx 15.38\\%$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "# or \n",
    "p_password_given_spam = spam['password']/sum(spam.values())\n",
    "print(p_password_given_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Given our dictionaries from the last activity, if we know an email is ham, what is the probability that the word \"password\" is in the email? \n",
    "\n",
    "What is the frequency of \"password\" in a ham email?\n",
    "\n",
    "- Answer:\n",
    "\n",
    "$P(password \\mid ham) = 1/(1+2+1+1+2+0) = 1/7 \\approx 14.29\\%$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "# or \n",
    "p_password_given_ham = ham['password']/sum(ham.values())\n",
    "print(p_password_given_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Assume we have seen the word \"password\" in an email, what is the probability that the email is spam?\n",
    "\n",
    "- $P(spam \\mid password) = ?$\n",
    "- Hint: Use Bayes' rule and Law of Total Probability (LOTP):\n",
    "    - Bayes' Rule: $P(spam \\mid password) = (P(password \\mid spam) P(spam))/ P(password)$ \n",
    "    - LOTP: $P(password) = P(password \\mid spam) P(spam) + P(password \\mid ham) P(ham)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Password:\n",
      "0.15018315018315018\n",
      "\n",
      "\n",
      "Probability of spam given password:\n",
      "0.6829268292682927\n"
     ]
    }
   ],
   "source": [
    "# Calculated by viewing our dataset\n",
    "p_spam = 4/6\n",
    "p_ham = 2/6\n",
    "\n",
    "# LOTP\n",
    "p_password = p_password_given_spam*p_spam + p_password_given_ham*p_ham \n",
    "print(\"Probability of Password:\")\n",
    "print(p_password)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Bayes Rule\n",
    "p_spam_given_password = p_password_given_spam*p_spam/p_password\n",
    "print(\"Probability of spam given password:\")\n",
    "print(p_spam_given_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier (Math)\n",
    "\n",
    "The Bayes Theorem : $P(spam | w_1, w_2, ..., w_n) = {P(w_1, w_2, ..., w_n | spam)P(spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "Naive Bayes assumption is that each word is independent of all other words, In reality, this is not true! But let's try it out for our spam/ham examples:\n",
    "\n",
    "Applying Bayes' Rule, the above relationship becomes simple for both spam and ham with the Naive Bayes assumption: \n",
    "\n",
    "$P(spam | w_1, w_2, ..., w_n) = {P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "$P(ham | w_1, w_2, ..., w_n) = {P(w_1| ham)P(w_2| ham) ... P(w_n| ham)P(ham)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "The denominator $P(w_1, w_2, ..., w_n)$ is independent of spam and ham, so we can remove it to simplify our equations, as we only care about labeling, and proportional relationships:\n",
    "\n",
    "$P(spam | w_1, w_2, ..., w_n) \\propto P(spam | w_1, w_2, ..., w_n) = {P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)}$\n",
    "\n",
    "$P(ham | w_1, w_2, ..., w_n) \\propto P(ham | w_1, w_2, ..., w_n) = {P(w_1| ham)P(w_2| ham) ... P(w_n| ham)P(ham)}$\n",
    "\n",
    "This is easier to express if we can write it as a summation. To do so, we can take the log of both sides of the equation, because the log of a product is the sum of the logs.\n",
    "\n",
    "$logP(spam | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)}$\n",
    "\n",
    "$logP(ham | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "**Given the above, we can therefore, say that if:**\n",
    "\n",
    "${\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)} > {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "**then that sentence is spam. Otherwise, the sentence is ham!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code for Naive Bayes for spam/ham dataset:\n",
    "\n",
    "- Assume the following small dataset is given\n",
    "\n",
    "- The first column is the labels of received emails\n",
    "\n",
    "- The second column is the body of the email (sentences)\n",
    "\n",
    "<img src=\"Images/spam_minidataset.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Based on the given dataset above, create the following two dictionaries:\n",
    "\n",
    "     Ham -> D_ham = {'Jos': 1,'ask':1, 'you':1,... }\n",
    "    \n",
    "     Spam- > D_spam= {'Did': 1, 'you':3, ... }\n",
    "    \n",
    "Each dictionary representes all words for the spam and ham emails and their frequency (as the value of dictionaries)\n",
    "\n",
    "2- For any new given sentences, having $w_1$, $w_2$, ... $w_n$ words, assuming the sentence is ham, calculate the following:\n",
    "\n",
    "     $P(w_1| ham)$, $P(w_2| ham)$, ..., $P(w_n| ham)$\n",
    "     $log(P(w_1| ham))$, $log(P(w_2| ham))$, ..., $log(P(w_n| ham))$\n",
    " \n",
    "then add them all together to create one value\n",
    "\n",
    "\n",
    "3- Calculate what percentage of labels are ham -> $P(ham)$ -> then take the log -> $log(P(ham))$\n",
    "\n",
    "4- Add the value from step (2) and (3)\n",
    "\n",
    "5- Do Steps (2) - (4) again, but assume the given new sentence is spam\n",
    "\n",
    "6- Compare the two values. The greater value indicates which label (class) the sentence should be given\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply the naive Bayes to spam/ham email dataset:\n",
    "\n",
    "**In groups of 3, complete the following activity**\n",
    "\n",
    "1. Please read this article, starting at the **Naive Bayes Assumption** section: https://pythonmachinelearning.pro/text-classification-tutorial-with-naive-bayes/\n",
    "1. We will use the [Spam Dataset](Datasets/spam.csv)\n",
    "1. In the article, for the codeblock of the `fit` method, which line(s) of the method calculates the probabilty of ham and spam?\n",
    "1. For the same `fit` method, which line(s) of the method calculates the spam and ham dictionaries?\n",
    "1. In the article, for the codeblock of the `predict` method, which line(s) compares the scores of ham or spam based on log probabilities?\n",
    "\n",
    "We will discuss as a class after workinging in groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Find the Naive Bayes core parts in the SpamDetector Class\n",
    "\n",
    "**In groups of 3, complete the following activity**\n",
    "\n",
    "Assume we have written the `SpamDetector` class from the article. Train this model from the given [Spam Dataset](Datasets/spam.csv), and use it to make a prediction!\n",
    "\n",
    "Use the starter code below, and then fill in the TODOs in the `main`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- you will need to use [train_test_split from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to obtain your training and test (prediction) data\n",
    "- You will need to instantiate your `SpamDetector`, fit the training data to it, predict using the test values, and then measure the accuracy\n",
    "- To calculate accuracy: add up all the correct predictions divided by the total number of predictions\n",
    "- Use the following code to get your data ready for transforming/manipulating:\n",
    "```\n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "X, y = texts, tags\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5572\n",
      "{'spam': 567, 'ham': 3612}\n",
      "0.9641\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "\n",
    "    # clean up our string by removing punctuation\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isn’t already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            \n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "        \n",
    "\n",
    "# TODO: Fill in the below function to make a prediction, \n",
    "# your answer should match the final number in the below output (0.9641)\n",
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # import/clean/label your data\n",
    "    data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "    data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "    data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "    print(data.head())\n",
    "    tags = data[\"label\"]\n",
    "    texts = data[\"text\"]\n",
    "\n",
    "    # create texts and tags\n",
    "    X, y = texts, tags\n",
    "    print(len(X))\n",
    "    \n",
    "    # transform text into numerical vectors\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    # instantiate your SpamDetector\n",
    "    MNB = SpamDetector()\n",
    "    # fit to model, with the trained part of the dataset\n",
    "    MNB.fit(X_train.values, y_train.values)\n",
    "    print(MNB.num_messages)\n",
    "#     print(MNB.word_counts)\n",
    "    # make predictions\n",
    "    pred = MNB.predict(X_test.values)\n",
    "    true = y_test.values\n",
    "    # test for accuracy\n",
    "    accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "    print(\"{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: use sklearn CountVectorizer and MultinomialNB to spam email dataset\n",
    "\n",
    "As we've seen with previous topics, sklearn has a lot of built in functionality that can save us from writing the code from scratch. We are going to solve the same problem in the previous activity, but using sklearn!\n",
    "\n",
    "For example, the `SpamDectector` class in the previous activity is an example of a **Multinomial Naive Bayes (MNB) model**. An MNB lets us know that each conditional probability we're looking at (i.e. $P(spam | w_1, w_2, ..., w_n)$) is a multinomial (several terms, polynomial) distribution, rather than another type distribution.\n",
    "\n",
    "**In groups of 3, complete the activity by using the provided starter code and following the steps below:**\n",
    "\n",
    "1 - Split the dataset\n",
    "\n",
    "`from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)`\n",
    "\n",
    "2 - Vectorize the dataset : `vect = CountVectorizer()`\n",
    "\n",
    "3 - Transform training data into a document-term matrix (BoW): `X_train_dtm = vect.fit_transform(X_train)`\n",
    "\n",
    "4 - Build and evaluate the model\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Remember how you prepared/cleaned/labeled the dataset, created texts and tags, and split the data innto train vs test from the previous activity. You'll need to do so again here\n",
    "- Review the [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see how you can transform text into numerical vectors\n",
    "- Need more help? Check out this [MNB Vectorization](https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/) article and see what you can use from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## starter code:\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# TODO: Prepare the dataset\n",
    "\n",
    "# TODO: create texts and tags\n",
    "\n",
    "# TODO: split the data into train vs test\n",
    "\n",
    "# TODO: transform text into numerical vectors\n",
    "\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  (0, 4527)\t1\n",
      "  (0, 3636)\t1\n",
      "  (0, 3647)\t1\n",
      "  (0, 3493)\t1\n",
      "  (0, 5683)\t1\n",
      "  (0, 3263)\t1\n",
      "  (0, 4469)\t1\n",
      "  (0, 2236)\t1\n",
      "  (0, 4798)\t1\n",
      "  (0, 1488)\t1\n",
      "  (0, 4854)\t1\n",
      "  (0, 3451)\t1\n",
      "  (0, 2218)\t1\n",
      "  (0, 6326)\t2\n",
      "  (0, 6604)\t1\n",
      "  (0, 1535)\t2\n",
      "  (0, 4176)\t2\n",
      "  (0, 7373)\t1\n",
      "  (0, 5065)\t2\n",
      "  (0, 2112)\t1\n",
      "  (0, 6727)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 819)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 919)\t1\n",
      "  :\t:\n",
      "  (4176, 4894)\t1\n",
      "  (4176, 4833)\t1\n",
      "  (4176, 3439)\t1\n",
      "  (4176, 1590)\t1\n",
      "  (4176, 4219)\t1\n",
      "  (4176, 7163)\t1\n",
      "  (4176, 4450)\t1\n",
      "  (4176, 6638)\t1\n",
      "  (4176, 2304)\t1\n",
      "  (4176, 3416)\t1\n",
      "  (4176, 3252)\t1\n",
      "  (4176, 4747)\t1\n",
      "  (4177, 6953)\t1\n",
      "  (4177, 5232)\t1\n",
      "  (4177, 1848)\t1\n",
      "  (4177, 4766)\t1\n",
      "  (4177, 3162)\t1\n",
      "  (4177, 4125)\t1\n",
      "  (4177, 6074)\t1\n",
      "  (4177, 3252)\t1\n",
      "  (4177, 3647)\t1\n",
      "  (4178, 1695)\t1\n",
      "  (4178, 6055)\t1\n",
      "  (4178, 7295)\t1\n",
      "  (4178, 4274)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856424982053122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare the dataset\n",
    "data = pd.read_csv('Datasets/spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "\n",
    "# create texts and tags\n",
    "X, y = texts, tags\n",
    "\n",
    "# split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "print(X_train_dtm)\n",
    "\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [MNB Vectorization](https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
