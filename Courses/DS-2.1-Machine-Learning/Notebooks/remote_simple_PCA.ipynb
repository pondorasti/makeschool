{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/learning_objectives.png\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principel Component Analysis (PCA)\n",
    "\n",
    "- PCA is a well-known algorithm for Dimensionality Reduction\n",
    "\n",
    "- PCA: \n",
    "\n",
    "    - Reduces the number of features while keeping the features information \n",
    "    \n",
    "    - Removes correlations among features\n",
    "    \n",
    "    - Emphasizes variation of strong features, making the data easier to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check in for Pre-Watching of PCA:\n",
    "\n",
    "Going forward, it is assumed that you have already watched the following videos:\n",
    "\n",
    "- What is PCA?: https://www.youtube.com/watch?v=HMOI_lkzW08 \n",
    "\n",
    "- What is a covariance matrix?: https://www.youtube.com/watch?v=0GzMcUy7ZI0\n",
    "\n",
    "- How to multiply a matrix with a vector?: https://www.youtube.com/watch?v=Awcj447pYuk \n",
    "\n",
    "Are there any questions about these videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review matrix multiplication\n",
    "\n",
    "- Matrix `A = np.array([[2, 0], [1, 5]])` and vector `v = np.array([3, 4])` are given.\n",
    "\n",
    "- **Question:** What is the multiplication of `A` by `v`?\n",
    "\n",
    "Solve using the following methods:\n",
    "\n",
    "1. Compute it by hand\n",
    "\n",
    "1. Write a Python function to compute it (Hint: use the following function form`numpy`: `np.dot(A, v)`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 23]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 0], [1, 5]])\n",
    "v = np.array([3, 4])\n",
    "\n",
    "print(np.dot(A, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EigenValue and Eigenvector of matrix\n",
    "\n",
    "A martix's eigenvalues and eigenvectors are what we will use for the scalar value `a` and vector `v` respectively.\n",
    "\n",
    "- **Eigenvector** (`v`) is a vector whose direction remains unchanged when a linear transformation is applied to it. They represent the rotation matrix\n",
    "- **Eigenvalues** (`a`) represents the scalar value that is used such that when multiplied by `v`, gives the same value as `Av`\n",
    "\n",
    "For given matrix `A`, we want to obtain a vector `v` and a scalar value `a` such that:\n",
    "\n",
    "`Av = av`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Python function to obtain vector `v` and scalar `a` for a given matrix `A`\n",
    "\n",
    "You will use the same matrix `A` that we used above.\n",
    "\n",
    "**hints:** \n",
    "\n",
    "1. Before we find the vector and scalar, we need the eigenvalue and eigenvector of `A`. Given the same matrix `A` we used above, see how [numpy's `linalg.eig`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html) method could help you solve this\n",
    "1. To check your answer, multiply `A` by one of its vectors, and then multiply `a` by the same vector, and see if you get the same outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 2.]\n",
      "[[ 0.          0.9486833 ]\n",
      " [ 1.         -0.31622777]]\n"
     ]
    }
   ],
   "source": [
    "eig_value, eig_vector = np.linalg.eig(A)\n",
    "\n",
    "print(eig_value)\n",
    "print(eig_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, matrix A has two eigen-values and two eigen-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Av = av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply A with its first eigen-vector\n",
    "np.dot(A, eig_vector[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 5.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply the one eigen-value of A with its associated eigen-vector\n",
    "eig_value[0]*eig_vector[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8973666 , -0.63245553])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly, multiply A with its second eigen-vector\n",
    "np.dot(A, eig_vector[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8973666 , -0.63245553])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply the other eigen-value of A with its associated eigen-vector\n",
    "eig_value[1]*eig_vector[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Are the countries in great UK different in terms of food?\n",
    "\n",
    "- In the table is the average consumption of 17 types of food in grams per person per week for every country in the UK\n",
    "- We want to visually represent the diffrence among UK countries based on the food they eat, but this can be difficult when there's 17 types of food (dimensions) to consider. The graph would be incredibly hard to read!\n",
    "- This is where PCA comes in to play: through PCA, we can consolidate the 17 types into what we call principle components. \n",
    "- **Principle Components** allow us to take an arbitrary number of data points (let's say 17) and consolidate them into a single (x, y) datapoint for a given feature.\n",
    "\n",
    "<img src=\"Images/pca_UK.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "## Question:\n",
    "\n",
    " - Which country is different from the the others? Any idea or reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it together: Write a function that obtains the principle components from 17 types of food in UK\n",
    "\n",
    "**Get in groups of 3 for this activity**\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "- Download the dataset we will use for this activity: [pca_uk](./Datasets/pca_uk.xlsx)\n",
    "- Run the following in your terminal:\n",
    "    - `conda install -c anaconda xlrd`\n",
    "    - `pip3 install xlrd`\n",
    "\n",
    "We will use two principle components as an example to see them visually, but we can pick 3 or more principle components as well\n",
    "\n",
    "**Outline to follow:**\n",
    "\n",
    "- use pandas to read in the excel spreadsheet (research how pandas can read an excel file)\n",
    "- build a matrix of the feature values, not including the text labels\n",
    "- calculate the PCA. This [sklearn module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) may be useful\n",
    "- Obtain the principle components. This can be done by [applying the dimensionality reduction onto our matrix](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 375   57  245 1472  105   54  193  147 1102  720  253  685  488  198\n",
      "   360 1374  156]\n",
      " [ 135   47  267 1494   66   41  209   93  674 1033  143  586  355  187\n",
      "   334 1506  139]\n",
      " [ 458   53  242 1462  103   62  184  122  957  566  171  750  418  220\n",
      "   337 1572  147]\n",
      " [ 475   73  227 1582  103   64  235  160 1137  874  265  803  570  203\n",
      "   365 1256  175]]\n",
      "[[-144.99315218   -2.53299944]\n",
      " [ 477.39163882  -58.90186182]\n",
      " [ -91.869339    286.08178613]\n",
      " [-240.52914764 -224.64692488]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use pandas to read in the excel spreadsheet\n",
    "df = pd.read_excel('./Datasets/pca_uk.xlsx')\n",
    "\n",
    "# build a matrix of the feature values, not including the text labels\n",
    "X = np.array([df[i].values for i in df.columns if i != 'Features'])\n",
    "\n",
    "print(X)\n",
    "\n",
    "# calculate the PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Find the principle components of 17 features\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "print(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAD4CAYAAAA3kTv/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbwUlEQVR4nO3de3RUZZrv8e9jAoFuwWgHBIEjOAMo14DBC9i2ggpqK9hLhR67RbywjgKKMj0HmjWKrsM6eGkV1B6bPiq6DiMggoKXAVGmbQZEEu4R0bQXLkYI05CGNlxCnvNH7cQCAgRTeasq/D5r1cre7/vuXU9RyC/73a9V5u6IiIiEcEqyCxARkZOHQkdERIJR6IiISDAKHRERCUahIyIiwWQmu4CayMnJ8bZt2ya7DBGRtFJQULDD3Zslu454aRE6bdu2JT8/P9lliIikFTP7Otk1HK7W02tm1sjMPjazNWZWaGYPR+3tzGy5mRWZ2Uwzaxi1Z0X7RVF/29rWICIi6SER93T2AX3dvTuQCwwws4uAR4Gn3P0fgZ3AHdH4O4CdUftT0TipQxMnTqRz585069aN3Nxcli9ffkLHr169mnfeeadqf9q0aYwcOTIhtU2YMIEnnngiIecSkdRX69DxmD3RboPo4UBfYHbU/jIwKNoeGO0T9fczM6ttHVK9ZcuW8dZbb7Fy5UrWrl3LokWLaNOmzQmd4/DQERH5oRKyes3MMsxsNbAdeA/4C7DL3cujIVuAVtF2K2AzQNRfCvykmnMON7N8M8svKSlJRJknpeLiYnJycsjKygIgJyeHs846ixUrVtC7d2+6d+/OBRdcwO7du9m7dy/Dhg2ja9eu9OjRg8WLF7N//34efPBBZs6cSW5uLjNnzjzk/PPnz+fCCy+kR48eXHHFFWzbtg2IXcHcfvvtXHbZZZxzzjlMmTKl6piJEyfSoUMHLrnkEjZu3BjuD0NEki4hoePuB909F2gNXACcm4BzTnX3PHfPa9YspRZfpJWrrrqKzZs306FDB+655x7+9Kc/sX//fgYPHszkyZNZs2YNixYtonHjxjz33HOYGevWrePVV19l6NChVFRU8MgjjzB48GBWr17N4MGDDzn/JZdcwkcffcSqVasYMmQIjz32WFXfp59+yoIFC/j44495+OGHOXDgAAUFBcyYMaPq6mnFihWh/0hEJIkSunrN3XeZ2WLgYiDbzDKjq5nWwNZo2FagDbDFzDKB04D/TmQdAm9/8TaTV07m279/y5njzqTvwb7s3biXwYMHM378eFq2bEmvXr0AaNq0KQBLlixh1KhRAJx77rmcffbZfPbZZ8d8ni1btjB48GCKi4vZv38/7dq1q+q79tprycrKIisri+bNm7Nt2zb+/Oc/c8MNN/CjH/0IgOuvv74uXr6IpKhErF5rZmbZ0XZj4EpgA7AYuDEaNhR4M9qeF+0T9X/g+qjrhHr7i7eZsHQCxX8vxnG+3fstcyrmcMHQC3j22WeZM2dOwp5r1KhRjBw5knXr1vGHP/yBvXv3VvVVTukBZGRkUF5eXt0pROQkkojptZbAYjNbC6wA3nP3t4D/BTxgZkXE7tm8EI1/AfhJ1P4AMDYBNUicySsns/dg7B//fcX72PftPvYe3MvklZNZvXo15513HsXFxVVTW7t376a8vJyf/vSnTJ8+HYDPPvuMTZs20bFjR5o0acLu3burfa7S0lJatYrdrnv55ZerHRPv0ksv5Y033qCsrIzdu3czf/78RLxkEUkTtZ5ec/e1QI9q2r8gdn/n8Pa9wE21fV45um///m3VdsW+Cr75f99Q8V0FRacUcWqvU5k6dSrDhg1j1KhRlJWV0bhxYxYtWsQ999zD3XffTdeuXcnMzGTatGlkZWVx+eWXM2nSJHJzcxk3btwhzzVhwgRuuukmTj/9dPr27cuXX355zNp69uzJ4MGD6d69O82bN6+a4hORk4Olw8xWXl6e6xMJau6q2VdR/PfiI9pb/rglC29cmISKRCQZzKzA3fOSXUc8feBnPXRfz/tolNHokLZGGY24r+d9SapIRCQmLT57TU7MtedcC1C1eq3Fj1twX8/7qtpFRJJFoVNPXXvOtQoZEUk5ml4TEZFgFDoiIhKMQkdERIJR6IiISDAKHRERCUahIyIiwSh0REQkGIWOiIgEo9AREZFgFDoiIhKMQkdERIJR6IiISDAKHRERCUahIyIiwSh0REQkGIWOiIgEo9AREZFgFDoiIhKMQkdERIJR6IiISDAKHRERCUahIyIiwSh0REQkGIWOiIgEo9AREZFgah06ZtbGzBab2SdmVmhm90XtZ5jZe2b2efTz9KjdzGyKmRWZ2Voz61nbGkREJD0k4kqnHBjj7p2Ai4ARZtYJGAu87+7tgfejfYCrgfbRYzjwbwmoQURE0kCtQ8fdi919ZbS9G9gAtAIGAi9Hw14GBkXbA4FXPOYjINvMWta2DhERSX0JvadjZm2BHsBy4Ex3L466vgXOjLZbAZvjDtsStYmISD2XsNAxs1OB14HR7v63+D53d8BP8HzDzSzfzPJLSkoSVaaIiCRRQkLHzBoQC5zp7j4nat5WOW0W/dwetW8F2sQd3jpqO4S7T3X3PHfPa9asWSLKFBGRJEvE6jUDXgA2uPuTcV3zgKHR9lDgzbj2W6NVbBcBpXHTcCIiUo9lJuAcfYBfA+vMbHXU9ltgEjDLzO4AvgZujvreAa4BioDvgGEJqEFERNJArUPH3ZcAdpTuftWMd2BEbZ9XRETSjz6RQEREglHoiIhIMAodEREJRqEjIiLBKHRERCQYhY6IiASj0BERkWAUOiIiEoxCR0REglHoiIhIMAodEREJRqEjIiLBKHRERCQYhY6IiASj0BERkWAUOiIiEoxCR0REglHoiIhIMAodEREJRqEjIiLBKHRERCQYhY6IiASj0BERkWAUOiIiEoxCR0REglHoiIhIMAodEREJRqEjIiLBKHRERCQYhY6IiASTkNAxsxfNbLuZrY9rO8PM3jOzz6Ofp0ftZmZTzKzIzNaaWc9E1CAiIqkvUVc604ABh7WNBd539/bA+9E+wNVA++gxHPi3BNUgIiIpLiGh4+4fAn89rHkg8HK0/TIwKK79FY/5CMg2s5aJqENERFJbXd7TOdPdi6Ptb4Ezo+1WwOa4cVuitkOY2XAzyzez/JKSkjosU0REQgmykMDdHfATPGaqu+e5e16zZs3qqDIREQmpLkNnW+W0WfRze9S+FWgTN6511CYiIvVcXYbOPGBotD0UeDOu/dZoFdtFQGncNJyIiNRjmYk4iZm9ClwG5JjZFuAhYBIwy8zuAL4Gbo6GvwNcAxQB3wHDElGDiIikvoSEjrv/8ihd/aoZ68CIRDyviIikF30igYiIBKPQERGRYBQ6IiISjEJHRESCUeiIiEgwCh0REQlGoSMiIsEodEREJBiFjoiIBKPQERGRYBQ6IiISjEJHRESCUeiIiEgwCh0REQlGoSMiIsEodEREJBiFjoiIBKPQERGRYBQ6IiISjEJHRESCUeiIiEgwCh0REQlGoSMiIsEodEREJBiFjoiIBKPQERGRYBQ6IiISjEInRWRkZJCbm1v1mDRp0g8+16mnnpqQmr766iu6dOmSkHOJiABkJrsAiWncuDGrV69OdhkiInVKVzoprm3btjz00EP07NmTrl278umnnwJQUlLClVdeSefOnbnzzjs5++yz2bFjxyHH7tmzh379+lUd++abbwKxK5jzzjuPu+66i86dO3PVVVdRVlYGQEFBAd27d6d79+4899xzYV+siNR7SQsdMxtgZhvNrMjMxiarjlRRVlZ2yPTazJkzq/pycnJYuXIld999N0888QQADz/8MH379qWwsJAbb7yRTZs2HXHORo0aMXfuXFauXMnixYsZM2YM7g7A559/zogRIygsLCQ7O5vXX38dgGHDhvHMM8+wZs2aAK9aRE42SZleM7MM4DngSmALsMLM5rn7J8moJ1neWLWVxxds5JtdZZDZkAkvvc2gHq2OGPeLX/wCgPPPP585c+YAsGTJEubOnQvAgAEDOP300484zt357W9/y4cffsgpp5zC1q1b2bZtGwDt2rUjNze36rxfffUVu3btYteuXVx66aUA/PrXv+bdd99N/AsXkZNWsu7pXAAUufsXAGY2AxgInDSh88aqrYybs46yAwcBcIdxc9YBHBE8WVlZQGyxQXl5eY2fY/r06ZSUlFBQUECDBg1o27Yte/fuPeScleetnF4TEalLyZpeawVsjtvfErVVMbPhZpZvZvklJSVBiwvh8QUbqwKnUtmBgzy+YGONju/Tpw+zZs0CYOHChezcufOIMaWlpTRv3pwGDRqwePFivv7662OeMzs7m+zsbJYsWQLEQktEJJFSdiGBu0919zx3z2vWrFmyy0m4b3YdemXh5fv55qVRrHjqTnJzcxk79ti3uR566CEWLlxIly5deO2112jRogVNmjQ5ZMwtt9xCfn4+Xbt25ZVXXuHcc889bl0vvfQSI0aMIDc3t+r+j4hIolgy/mExs4uBCe7eP9ofB+Du/6e68Xl5eZ6fnx+wwrrXZ9IHbN115JRWq+zG/NfYvsc9ft++fWRkZJCZmcmyZcu4++67teRaRA5hZgXunpfsOuIl657OCqC9mbUDtgJDgH9KUi1J8Zv+HQ+5pwPQuEEGv+nfsUbHb9q0iZtvvpmKigoaNmzIH//4x7oqVUQkYZISOu5ebmYjgQVABvCiuxcmo5ZkqVwsULl67azsxvymf8dqV69Vp3379qxataouSxQRSbikTK+dqPo4vSYiUtdScXotZRcSiIhI/aPQERGRYBQ6IiISjEJHRESCUeiIiEgwCh0REQlGoSMiIsEodEREJBiFjoiIBKPQERFJcWbGmDFjqvafeOIJJkyYcMS4adOmMXLkyBM99wQz++fa1hida5qZ3XisMQodEZEUl5WVxZw5c9ixY8cPOt7MkvXhzkdQ6IiIpLjMzEyGDx/OU089VeNjbrvtNoD/YWbLgcfM7B/M7D/MrMDM/mxmR3zBlpndZWYrzGyNmb1uZj+K2qeZ2RQzW2pmX1RezVjMs2a20cwWAc2PV5dCR0QkDYwYMYLp06dTWlp6Ioc1BHq7+wPAVGCUu58P/DPw+2rGz3H3Xu7eHdgA3BHX1xK4BPg5MClquwHoCHQCbgV6H6+glLnkEhGRo2vatCm33norU6ZMoXHjxjU9bKe7HzSzU4kFwmtmVtmXVc34Lmb2v4Fs4FRiXz9T6Q13rwA+MbMzo7ZLgVfd/SDwjZl9cLyCFDoiIqlo7Sx4/xEo3QIHymDtLEaPHk3Pnj0ZNmxYTc9SEf08Bdjl7rnHGT8NGOTua8zsNuCyuL59cdvGD6TpNRGRVLN2Fsy/F0o3Aw5eAfPv5Ywti7j55pt54YUXTuh07v434Eszuwmq7sV0r2ZoE6DYzBoAt9Tg1B8Cg80sw8xaApcf7wCFjohIqnn/kdjVTbwDZfD+I4wZM+aHrmK7BbjDzNYAhcDAasb8K7Ac+C/g0xqccy7wOfAJ8Aqw7HgH6JtDRURSzYRsoLp/mw0m7KrxafTNoSIicnyntT6x9jSi0BERSTX9HoQGh61Qa9A41p7mFDoiIqmm281w3RQ4rQ1gsZ/XTYm1pzktmRYRSUXdbq4XIXM4XemIiEgwCh0REQlGoSMiIsEodEREJBiFjoiIBKPQERGRYBQ6IiISjEJHRESCqVXomNlNZlZoZhVmlndY3zgzK4q+xrR/XPuAqK3IzMbW5vlFRCS91PZKZz3wC2LfqVDFzDoBQ4DOwADg99H3LWQAzwFXE/t6019GY0VE5CRQq4/BcfcNAHFff1ppIDDD3fcR++KgIuCCqK/I3b+IjpsRjf2kNnWIiEh6qKt7Oq2AzXH7W6K2o7UfwcyGm1m+meWXlJTUUZkiIhLSca90zGwR0KKarvHu/mbiS4px96nAVIh9iVtdPY+IiIRz3NBx9yt+wHm3Am3i9ltHbRyjXURE6rm6ml6bBwwxsywzawe0Bz4GVgDtzaydmTUktthgXh3VICIiKaZWCwnM7AbgGaAZ8LaZrXb3/u5eaGaziC0QKAdGuPvB6JiRwAIgA3jR3Qtr9QpERCRtmHvq3y7Jy8vz/Pz8ZJchIpJWzKzA3fOOPzIcfSKBiIgEo9AREZFgFDoiIhKMQkdERIJR6IiISDAKHRERCUahIyIiwSh0REQkGIWOiIgEo9AREZFgFDoiIhKMQkdERIJR6IiISDAKHRERCUahIyIiwSh0REQkGIWOiIgEo9AREZFgFDoiIhKMQkdERIJR6IiISDAKHRERCUahIyIiwSh0REQkGIWOiIgEo9AREZFgFDoiIhKMQkdERIJR6IiISDC1Ch0ze9zMPjWztWY218yy4/rGmVmRmW00s/5x7QOitiIzG1ub5xcRkfRS2yud94Au7t4N+AwYB2BmnYAhQGdgAPB7M8swswzgOeBqoBPwy2isiIicBGoVOu6+0N3Lo92PgNbR9kBghrvvc/cvgSLgguhR5O5fuPt+YEY0VkRETgKJvKdzO/ButN0K2BzXtyVqO1r7EcxsuJnlm1l+SUlJAssUEZFkyTzeADNbBLSopmu8u78ZjRkPlAPTE1WYu08FpgLk5eV5os4rIiLJc9zQcfcrjtVvZrcBPwf6uXtlOGwF2sQNax21cYx2ERGp52q7em0A8C/A9e7+XVzXPGCImWWZWTugPfAxsAJob2btzKwhscUG82pTQ03cf//9PP3001X7/fv3584776zaHzNmDE8++WS1x952223Mnj27rksUETkp1PaezrNAE+A9M1ttZs8DuHshMAv4BPgPYIS7H4wWHYwEFgAbgFnR2DrVp08fli5dCkBFRQU7duygsPD7p126dCm9e/eu6zJERE56tV299o/u3sbdc6PH/4zrm+ju/+DuHd393bj2d9y9Q9Q3sTbPX1O9e/dm2bJlABQWFtKlSxeaNGnCzp072bdvHxs2bGDhwoX06tWLLl26MHz4cL6fKfxeQUEBP/vZzzj//PPp378/xcXFAEyZMoVOnTrRrVs3hgwZEuIliYikpZPiEwnOOussMjMz2bRpE0uXLuXiiy/mwgsvZNmyZeTn59O1a1dGjhzJihUrWL9+PWVlZbz11luHnOPAgQOMGjWK2bNnU1BQwO2338748eMBmDRpEqtWrWLt2rU8//zzyXiJIiJp4bgLCdJZ6fz5bH/qacqLi+laVsaiZ55h6fbtPPDAA2zdupWlS5dy2mmn0adPHxYvXsxjjz3Gd999x1//+lc6d+7MddddV3WujRs3sn79eq688koADh48SMuWLQHo1q0bt9xyC4MGDWLQoEFJea0iIumg3oZO6fz5FP/rg/jevQDkOiyeNo21TZvS5cUXadOmDb/73e9o2rQpw4YN46677iI/P582bdowYcIE9kbHVXJ3OnfuXDVNF+/tt9/mww8/ZP78+UycOJF169aRmVlv/2hFRH6weju9tv2pp6sCByC3cWP+s7SUH+/cSUZGBmeccQa7du1i2bJlVYsIcnJy2LNnT7Wr1Tp27EhJSUlV6Bw4cIDCwkIqKirYvHkzl19+OY8++iilpaXs2bMnzIsUEUkz9fbX8fLoJn+lDllZ7Dx4kGtP+T5nu3btyp49e8jJyeGuu+6iS5cutGjRgl69eh1xvoYNGzJ79mzuvfdeSktLKS8vZ/To0XTo0IFf/epXlJaW4u7ce++9ZGdnH3G8iIiAVbdKK9Xk5eV5fn7+CR3zed9+lH/zzRHtmWedRfsP3k9UaSIiKcvMCtw9L9l1xKu302vN7x+NNWp0SJs1akTz+0cnqSIREam302unRSvPKlevZbZsSfP7R1e1i4hIePU2dCAWPAoZEZHUUW+n10REJPUodEREJBiFjoiIBKPQERGRYBQ6IiISTFr8z6FmVgJ8neQycoAdSa6hJtKlTkifWlVn4qVLrelSJ1Rf69nu3iwZxRxNWoROKjCz/FT7P3urky51QvrUqjoTL11qTZc6IX1q1fSaiIgEo9AREZFgFDo1NzXZBdRQutQJ6VOr6ky8dKk1XeqENKlV93RERCQYXemIiEgwCh0REQlGoXMYM3vczD41s7VmNtfMsuP6xplZkZltNLP+ce0DorYiMxsbsNabzKzQzCrMLO+wvpSq9bDakl7DYfW8aGbbzWx9XNsZZvaemX0e/Tw9ajczmxLVvtbMegass42ZLTazT6L3/b5UrNXMGpnZx2a2Jqrz4ai9nZktj+qZaWYNo/asaL8o6m8bos64ejPMbJWZvZXidX5lZuvMbLWZ5UdtKfXe14i76xH3AK4CMqPtR4FHo+1OwBogC2gH/AXIiB5/Ac4BGkZjOgWq9TygI/CfQF5ce8rVGldb0muopqZLgZ7A+ri2x4Cx0fbYuL8H1wDvAgZcBCwPWGdLoGe03QT4LHqvU6rW6PlOjbYbAMuj558FDInanwfujrbvAZ6PtocAMwO//w8A/w68Fe2nap1fATmHtaXUe1+Th650DuPuC929PNr9CGgdbQ8EZrj7Pnf/EigCLogeRe7+hbvvB2ZEY0PUusHdN1bTlXK1xkmFGg7h7h8Cfz2seSDwcrT9MjAorv0Vj/kIyDazloHqLHb3ldH2bmAD0CrVao2eb0+02yB6ONAXmH2UOivrnw30MzOr6zoBzKw1cC3wf6N9S8U6jyGl3vuaUOgc2+3EfluA2H/cm+P6tkRtR2tPplSuNRVqqIkz3b042v4WODPaTon6o6mdHsSuIlKu1mjKajWwHXiP2NXtrrhf6OJrqaoz6i8FfhKiTuBp4F+Aimj/JylaJ8SCe6GZFZjZ8Kgt5d7746nX3xx6NGa2CGhRTdd4d38zGjMeKAemh6ztcDWpVeqWu7uZpcz/W2BmpwKvA6Pd/W/xv2ynSq3ufhDIje6JzgXOTXJJRzCznwPb3b3AzC5Ldj01cIm7bzWz5sB7ZvZpfGeqvPfHc1KGjrtfcax+M7sN+DnQz6MJUmAr0CZuWOuojWO019rxaj2KpNRaQ8eqLZVsM7OW7l4cTUtsj9qTWr+ZNSAWONPdfU4q1wrg7rvMbDFwMbEpnszoKiG+lso6t5hZJnAa8N8ByusDXG9m1wCNgKbA5BSsEwB33xr93G5mc4lNVafse380ml47jJkNIHa5fb27fxfXNQ8YEq1gaQe0Bz4GVgDtoxUvDYndYJwXuu7DpHKtqVBDTcwDhkbbQ4E349pvjVYHXQSUxk1v1Kno/sELwAZ3fzJVazWzZtEVDmbWGLiS2P2nxcCNR6mzsv4bgQ/iftmrM+4+zt1bu3tbYn8PP3D3W1KtTgAz+7GZNancJrbgaT0p9t7XSLJXMqTag9hN983A6ujxfFzfeGJz0xuBq+ParyG2kugvxKa9QtV6A7G52n3ANmBBqtZ6WN1Jr+Gwel4FioED0Z/nHcTm6t8HPgcWAWdEYw14Lqp9HXGrBgPUeQmxef21cX8/r0m1WoFuwKqozvXAg1H7OcR++SkCXgOyovZG0X5R1H9OEv4OXMb3q9dSrs6opjXRo7Dyv5tUe+9r8tDH4IiISDCaXhMRkWAUOiIiEoxCR0REglHoiIhIMAodEREJRqEjIiLBKHRERCSY/w9O3nGbnNf0QQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets visualize the principle components\n",
    "\n",
    "for feature, (plot_x,plot_y) in enumerate(zip(X_r[:, 0], X_r[:, 1])):\n",
    "    plt.scatter(plot_x, plot_y)\n",
    "    plt.text(plot_x+0.3, plot_y+0.3, df.columns[:-1][feature])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: Ireland is different from other three countries in UK \n",
    "\n",
    "Why is Ireland such an outlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much of the information in the original dataset is preserved in the principle components?\n",
    "\n",
    "**Hint:** use [`pca.explained_variance_ratio_`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-144.99315218   -2.53299944]\n",
      " [ 477.39163882  -58.90186182]\n",
      " [ -91.869339    286.08178613]\n",
      " [-240.52914764 -224.64692488]]\n",
      "[105073.34576714  45261.62487597]\n",
      "[0.67444346 0.29052475]\n",
      "[0.67444346 0.96496821]\n"
     ]
    }
   ],
   "source": [
    "# PCA computation by sklearn\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(X)\n",
    "print(X_r)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to calculate the correlation of the principle components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of PCA Component:\n",
      "(0.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print('Correlation of PCA Component:')\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets write our own function to obtain principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: PCA Steps\n",
    "\n",
    "**In groups of 3:** Follow the steps here and write a function that computes the principle components for a dataset similar to the one we watched on YouTube: https://www.youtube.com/watch?v=0GzMcUy7ZI0 \n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Use the following matrix: `X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])`\n",
    "1. Subtract the column mean from the feature matrix -> this new matrix will be our centered matrix\n",
    "1. Calculate the covariance of the centered matrix (check out numpy's resources to see if there's a function that can do this for you...) --> this new matrix will be our covariance matrix.\n",
    "1. Calculate the eigenvalue and eigenvector of the covariance matrix. Remember how we did this in a previous activity!\n",
    "1. Sort the eigevalues so that they are in decresing order, and then find the top N (for example, 2) eigenvectors \n",
    "1. Dot multiply the centered matrix with the top N eigenvectors of the covariance matrix \n",
    "1. Compare the result of custom function with PCA in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.65392786 -0.2775295 ]\n",
      " [-0.84584087  0.31153366]\n",
      " [ 0.55130929  0.09250983]\n",
      " [ 1.94845944 -0.126514  ]]\n",
      "[2.5171201  0.06621324]\n",
      "[0.97436907 0.02563093]\n",
      "[0.97436907 1.        ]\n",
      "Correlation of PCA Component:\n",
      "(3.885780586188048e-16, 0.9999999999999996)\n",
      "[[0.         0.         0.        ]\n",
      " [0.         1.66666667 1.16666667]\n",
      " [0.         1.16666667 0.91666667]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.65392786, -0.2775295 ],\n",
       "       [ 0.84584087,  0.31153366],\n",
       "       [-0.55130929,  0.09250983],\n",
       "       [-1.94845944, -0.126514  ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "\n",
    "# PCA computation by sklearn\n",
    "\n",
    "X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])\n",
    "# print(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(X)\n",
    "print(X_r)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "print('Correlation of PCA Component:')\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))\n",
    "\n",
    "\n",
    "# Our function to compare \n",
    "def PCA_calculation(data, n_comp=2):\n",
    "    M = np.mean(data, axis=0)\n",
    "    # center columns by subtracting column means\n",
    "    C =  data - M\n",
    "    # calculate covariance matrix of centered matrix\n",
    "    V = np.cov(C.T)\n",
    "    print(V)\n",
    "    # eigen decomposition of covariance matrix\n",
    "    eig_value, eig_vector = np.linalg.eig(V)\n",
    "    # sort eigenvalue in decreasing order\n",
    "    idx = np.argsort(eig_value)[::-1] \n",
    "    idx_n_comp = idx[:n_comp]\n",
    "    # eigenvectors according to top n_comp largest\n",
    "    eig_vector = eig_vector[:, idx_n_comp]\n",
    "    P = np.dot(C, eig_vector)\n",
    "    return P\n",
    "\n",
    "\n",
    "PCA_calculation(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Is PCA Supervised or Unsupervised?\n",
    "\n",
    "- Did we use any label to do dimensionality reduction? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply Principle to Boston housing features and then train the linear regression model \n",
    "\n",
    "- Basically, we remove correlation among features with PCA\n",
    "\n",
    "- We do not need to do feature data scaling (normalization) when we do PCA for features, because \n",
    "\n",
    "- Report the R-squared and MSE for a system with PCA+Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "- PCA is a mathematical technique to reduce redundancy in data, and is an algorithm for Dimensionality Reduction\n",
    "\n",
    "- PCA emphasizes variation and strong patterns, making the data easier to visualize\n",
    "\n",
    "- We use eigenvectors and eigenvalues to obtain the principle component (our new features) in lower dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "-  http://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.65392786 -0.2775295 ]\n",
      " [ 0.84584087  0.31153366]\n",
      " [-0.55130929  0.09250983]\n",
      " [-1.94845944 -0.126514  ]]\n"
     ]
    }
   ],
   "source": [
    "def PCA_custom(data, dims_rescaled_data=2):\n",
    "    \"\"\"\n",
    "    returns: data transformed in 2 dims/columns + regenerated original data\n",
    "    pass in: data as 2D NumPy array\n",
    "    \"\"\"\n",
    "    # mean center the data\n",
    "    data = data - np.mean(data, axis=0)\n",
    "    # calculate the covariance matrix\n",
    "    R = np.cov(data, rowvar=False)\n",
    "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
    "    # use 'eigh' rather than 'eig' since R is symmetric,\n",
    "    # the performance gain is substantial\n",
    "    evals, evecs = np.linalg.eig(R)\n",
    "    # sort eigenvalue in decreasing order\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evecs = evecs[:, idx]\n",
    "    # sort eigenvectors according to same index\n",
    "    evals = evals[idx]\n",
    "    # select the first n eigenvectors (n is desired dimension\n",
    "    # of rescaled data array, or dims_rescaled_data)\n",
    "    evecs = evecs[:, :dims_rescaled_data]\n",
    "    # carry out the transformation on the data using eigenvectors\n",
    "    # and return the re-scaled data, eigenvalues, and eigenvectors\n",
    "    return np.dot(evecs.T, data.T).T\n",
    "\n",
    "\n",
    "print(PCA_custom(X, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
