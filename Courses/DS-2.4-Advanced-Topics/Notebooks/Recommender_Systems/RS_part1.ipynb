{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems (RS):\n",
    "\n",
    "There are two basic architectures for a recommendation system:\n",
    "1. Content-Based systems focus on properties of items. Similarity of items\n",
    "is determined by measuring the similarity in their properties.\n",
    "2. Collaborative-Filtering systems focus on the relationship between users\n",
    "and items. Similarity of items is determined by the similarity of the\n",
    "ratings of those items by the users who have rated both items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based RS from Item Desciption: \n",
    "\n",
    "- The item description has been represented (encoded) by TFIDF\n",
    "\n",
    "- The similary between two items is computed by Cosine formula:\n",
    "\n",
    "<img src=\"cosine_formula.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "- When we have the features for the items, we can apply the same method to obtain the most similars items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import redis\n",
    "from flask import current_app\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "def info(msg):\n",
    "    current_app.logger.info(msg)\n",
    "\n",
    "\n",
    "class ContentEngine(object):\n",
    "\n",
    "    SIMKEY = 'p:smlr:%s'\n",
    "\n",
    "    def __init__(self):\n",
    "        self._r = redis.StrictRedis.from_url(current_app.config['REDIS_URL'])\n",
    "\n",
    "    def train(self, data_source):\n",
    "        start = time.time()\n",
    "        ds = pd.read_csv(data_source)\n",
    "        info(\"Training data ingested in %s seconds.\" % (time.time() - start))\n",
    "\n",
    "        # Flush the stale training data from redis\n",
    "        self._r.flushdb()\n",
    "\n",
    "        start = time.time()\n",
    "        self._train(ds)\n",
    "        info(\"Engine trained in %s seconds.\" % (time.time() - start))\n",
    "\n",
    "    def _train(self, ds):\n",
    "        \"\"\"\n",
    "        Train the engine.\n",
    "\n",
    "        Create a TF-IDF matrix of unigrams, bigrams, and trigrams\n",
    "        for each product. The 'stop_words' param tells the TF-IDF\n",
    "        module to ignore common english words like 'the', etc.\n",
    "\n",
    "        Then we compute similarity between all products using\n",
    "        SciKit Leanr's linear_kernel (which in this case is\n",
    "        equivalent to cosine similarity).\n",
    "\n",
    "        Iterate through each item's similar items and store the\n",
    "        100 most-similar. Stops at 100 because well...  how many\n",
    "        similar products do you really need to show?\n",
    "\n",
    "        Similarities and their scores are stored in redis as a\n",
    "        Sorted Set, with one set for each item.\n",
    "\n",
    "        :param ds: A pandas dataset containing two fields: description & id\n",
    "        :return: Nothin!\n",
    "        \"\"\"\n",
    "\n",
    "        tf = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range=(1, 3),\n",
    "                             min_df=0,\n",
    "                             stop_words='english')\n",
    "        tfidf_matrix = tf.fit_transform(ds['description'])\n",
    "\n",
    "        cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        for idx, row in ds.iterrows():\n",
    "            similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n",
    "            similar_items = [(cosine_similarities[idx][i], ds['id'][i])\n",
    "                             for i in similar_indices]\n",
    "\n",
    "            # First item is the item itself, so remove it.\n",
    "            # This 'sum' is turns a list of tuples into a single tuple:\n",
    "            # [(1,2), (3,4)] -> (1,2,3,4)\n",
    "            flattened = sum(similar_items[1:], ())\n",
    "            self._r.zadd(self.SIMKEY % row['id'], *flattened)\n",
    "\n",
    "    def predict(self, item_id, num):\n",
    "        \"\"\"\n",
    "        Couldn't be simpler! Just retrieves the similar items and\n",
    "        their 'score' from redis.\n",
    "\n",
    "        :param item_id: string\n",
    "        :param num: number of similar items to return\n",
    "        :return: A list of lists like: [[\"19\", 0.2203],\n",
    "        [\"494\", 0.1693], ...]. The first item in each sub-list is\n",
    "        the item ID and the second is the similarity score. Sorted\n",
    "        by similarity score, descending.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._r.zrange(self.SIMKEY % item_id,\n",
    "                              0,\n",
    "                              num-1,\n",
    "                              withscores=True,\n",
    "                              desc=True)\n",
    "\n",
    "content_engine = ContentEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering for RS : \n",
    "\n",
    "- Collaborative Filtering simply put uses the \"wisdom of the crowd\" to recommend items\n",
    "\n",
    "- We have item-item collaborative filtering and user-user collaborative filtering\n",
    "\n",
    "- In collaborative filtering, we have rating (utility) matrix as follows:\n",
    "\n",
    "<img src=\"user_user_rating.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "<img src=\"Pearson.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the similarity (Cosine) for users (A, B), (A, C) and (B, C)\n",
    "\n",
    "sim(A, B) = ...\n",
    "\n",
    "sim(A, C) = ...\n",
    "\n",
    "sim(B, C) = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The estimation for prediction of rating for user-user collaborative filtering: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{u,i}$ = $\\bar {r}_{u}$ + $\\frac{\\sum(r_{v,i} - \\bar {r}_{v})sim(u,v)}{\\sum |sim(u,v)|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The estimation for prediction of rating for item-item collaborative filtering: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_{u,i}$ = $\\bar {\\mu}_{i}$ + $\\frac{\\sum_{j \\in I_u}(r_{u,j} - \\bar {\\mu }_{j})w_{ij}}{\\sum_{j \\in I_u}{|w_{ij}|}}$\n",
    "\n",
    "$P_{u,i}$: The predicted rating of item $i$ for user $u$ \n",
    "\n",
    "$\\bar \\mu_i$, $\\bar \\mu_j$: The average ratings for item $i$ and item $j$ across all users respectively\n",
    "\n",
    "$I_u$: The set of items rated by user $u$\n",
    "\n",
    "$w_{ij}$: The similarity between item $i$ and item $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative RS in Python using the MovieLens Dataset\n",
    "\n",
    "- MovieLens Dataset is iris dataset for RS\n",
    "\n",
    "- MovieLens 100k: This is a classic small recommender dataset, 100,000 ratings (1-5) from 943 users on 1682 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "\n",
    "# pass in column names for each CSV as the column name is not given in the file and read them using pandas.\n",
    "# You can check the column names from the readme file\n",
    "\n",
    "#Reading users file:\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('ml-100k/u.user', sep='|', names=u_cols,encoding='latin-1')\n",
    "\n",
    "#Reading ratings file:\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=r_cols,encoding='latin-1')\n",
    "\n",
    "#Reading items file:\n",
    "i_cols = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    "'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "items = pd.read_csv('ml-100k/u.item', sep='|', names=i_cols,\n",
    "encoding='latin-1')\n",
    "\n",
    "# Users:\n",
    "print(users.shape)\n",
    "users.head()\n",
    "\n",
    "# Ratings\n",
    "print(ratings.shape)\n",
    "ratings.head()\n",
    "\n",
    "# Building collaborative filtering model from scratch\n",
    "n_users = ratings.user_id.unique().shape[0]\n",
    "n_items = ratings.movie_id.unique().shape[0]\n",
    "\n",
    "# Utility Matrix:\n",
    "\n",
    "data_matrix = np.zeros((n_users, n_items))\n",
    "for line in ratings.itertuples():\n",
    "    data_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "\n",
    "# use the pairwise_distance function from sklearn to calculate the cosine similarity.\n",
    "user_similarity = pairwise_distances(data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(data_matrix.T, metric='cosine')\n",
    "\n",
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #We use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred\n",
    "\n",
    "#  make predictions based on user similarity and item similarity\n",
    "user_prediction = predict(data_matrix, user_similarity, type='user')\n",
    "item_prediction = predict(data_matrix, item_similarity, type='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization for RS:\n",
    "\n",
    "- An entirely different approach to estimating the blank entries in the utility\n",
    "matrix is to conjecture that the utility matrix is actually the product of two\n",
    "long, thin matrices\n",
    "\n",
    "- This view makes sense if there are a relatively small set\n",
    "of features of items and users that determine the reaction of most users to\n",
    "most items.\n",
    "\n",
    "- http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#\n",
    "# Created by Albert Au Yeung (2010)\n",
    "#\n",
    "# An implementation of matrix factorization\n",
    "#\n",
    "try:\n",
    "    import numpy\n",
    "except:\n",
    "    print(\"This implementation requires the numpy module.\")\n",
    "    exit(0)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "@INPUT:\n",
    "    R     : a matrix to be factorized, dimension N x M\n",
    "    P     : an initial matrix of dimension N x K\n",
    "    Q     : an initial matrix of dimension M x K\n",
    "    K     : the number of latent features\n",
    "    steps : the maximum number of steps to perform the optimisation\n",
    "    alpha : the learning rate\n",
    "    beta  : the regularization parameter\n",
    "@OUTPUT:\n",
    "    the final matrices P and Q\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        eR = numpy.dot(P,Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)\n",
    "                    for k in range(K):\n",
    "                        e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    R = [\n",
    "         [5,3,0,1],\n",
    "         [4,0,0,1],\n",
    "         [1,1,0,5],\n",
    "         [1,0,0,4],\n",
    "         [0,1,5,4],\n",
    "        ]\n",
    "\n",
    "    R = numpy.array(R)\n",
    "\n",
    "    N = len(R)\n",
    "    M = len(R[0])\n",
    "    K = 2\n",
    "\n",
    "    P = numpy.random.rand(N,K)\n",
    "    Q = numpy.random.rand(M,K)\n",
    "\n",
    "    nP, nQ = matrix_factorization(R, P, Q, K)\n",
    "    print(nP)\n",
    "    print(nQ)\n",
    "    print(numpy.dot(nP, nQ.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "R = [\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "]\n",
    "\n",
    "R = numpy.array(R)\n",
    "\n",
    "nmf = NMF(n_components=2,  alpha=0.02, init='random')\n",
    "nmf.fit(R)\n",
    "\n",
    "print(nmf.components_)\n",
    "print(nmf.transform(R))\n",
    "print(numpy.dot(nmf.transform(R), nmf.components_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
