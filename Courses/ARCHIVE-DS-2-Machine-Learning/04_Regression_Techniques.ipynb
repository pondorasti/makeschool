{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Regression</h1></center>\n",
    "\n",
    "Of all the techniques in Machine Learning and Predictive Analytics, **_Regression_** is one of the most mature.  Recall that regression is different from the classification problems we have dealt with so far; whereas classification looks at data and tells us what class it belongs to, regression instead seeks to output a continuous value.  Classification and regression are often two sides of the same coin, and which one you use often depends on the phrasing of the question you're trying to answer.  For this exercise, we'll be trying to predict housing prices on the `boston_housing_prices` data set.  Let's look at an some example questions we could ask about this data set.\n",
    "\n",
    "<h3>Classification</h3>\n",
    "* Will this house sell for more or less than this price? *\n",
    "<br>\n",
    "* Does this house have less than 3 offers on it? *\n",
    "<br>\n",
    "* Based on the price and other data about the house, can we predict if this house has 2 bedrooms, or less, or more?*\n",
    "\n",
    "<h3>Regression</h3>\n",
    "* How much do we think this house will sell for? *\n",
    "<br>\n",
    "* Based on the data, how many offers do we expect this house to receive? *\n",
    "<br>\n",
    "* Based on price and other data about the house, can we predict how many bedrooms a house has? *\n",
    "<br>\n",
    "<br>\n",
    "As you can see, these are pretty much the same question, but phrased differently.  Under the hood, classification is often just a regression, with the caveat that values below the threshold are one class while values above the threshold are another.  \n",
    "\n",
    "<center><img src='img/classification_vs_regression.png' height=40% width=40%></center>\n",
    "\n",
    "<center><h2>The Basics: Linear Regression</h2></center>\n",
    "\n",
    "Before we begin coding, we'll briefly explore how it works and how we can interpret a regression.  This lesson will probably feel more math-heavy than other lessons, and will certainly contain more jargon--but don't worry! The math behind linear regression is fairly intuitive (and fairly simple), and regression is great baseline for understanding how more advanced topics in machine learning work, such as Neural Networks. \n",
    "\n",
    "<h3>The Idea Behind Regression</h3>\n",
    "\n",
    "The most basic form of regression is **_Linear Regression_**.  Linear Regression is a **_Linear Modeling_** technique used to predict a variable as a function of another variable. You've probably seen this written out like this before:\n",
    "\n",
    "<center><h3>y = f(x)</h3></center>\n",
    "\n",
    "The most common version of this function (and the one we're trying to discover in Linear Regression) is also one you're likely familiar with:\n",
    "\n",
    "<center><h3>y = mx + b</h3></center>\n",
    "\n",
    "This is the basic formula for a line on a cartesian plane that you likely learned in grade school.  In linear regression, our goal is to take some data: \n",
    "    \n",
    "\n",
    "<center><img src='img/scatter.png' height=40% width=40%></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>and discover the function for the **_line of best fit._**</center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src='img/lbf.png' height=60% width=60%></center>\n",
    "\n",
    "Let's take a look at the line for the equation and interpret what it means.  You'll often see the the equation for the line of best fit represented like this:\n",
    "\n",
    "<center><img src='img/regression_equation.png' height=40% width=40%></center>\n",
    "\n",
    "Let's examine what these symbols mean, and see if we can't \n",
    "\n",
    "**_α_**: This value is the bias.  Mathematically, we can think of this as where our line crosses the y-axis.  \n",
    "\n",
    "**_β_**: is the slope, which defines the relationship between x and y.  In layman's terms, we can interpret this value as how much we expect to y to increase whenever we increase the value of x by 1.  \n",
    "\n",
    "If we were trying to use linear regression to predict something like how much it would cost for a factory to produce \\[x\\] widgets, then we could use this equation to make a prediction. In this scenario:\n",
    "\n",
    "β would represent the cost per unit to produce a widget.  How much we have to spend on manufacturing depends on how many we want ot manufacture.  If we want to manufacture 10,000 widgets and they cost us $.80 each, then the value for β would be .8.\n",
    "\n",
    "α would represent the fixed costs that don't change with the scale of x. In this scenario, these could be fixed operating costs like how much rent we have to pay to occupy the factory.  There's no avoiding this cost, and it doesn't increase or decrease with our scale.  If the rent for our factory is $10,000/month, then we know we'll have to pay $10,000 this month, regardless of whether we make 1 widget or 1 million widgets.  \n",
    "\n",
    "\n",
    "<center><h3>Fitting a Linear Regression in Python</h3></center>\n",
    "\n",
    "In the code block below, we'll create a basic data set that contains some correlation between X and Y.  Then, we'll fit a linear regression to it, and display the line of best fit. Feel free to examine the code and try to figure out how it works, but don't worry if you don't understand everything just yet--this is just help us create a quick visualization of a line of best fit against a scatterplot of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_dataset(num_samples=100, noise=100):\n",
    "    '''Generates a random number of samples to act as a dataset for linear regression.'''\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(num_samples):\n",
    "        x.append(i)\n",
    "        if noise == 0:\n",
    "            added_noise  = 0\n",
    "        else:\n",
    "            upper_noise_limit = noise / 2\n",
    "            lower_noise_limit = (upper_noise_limit) * -1\n",
    "            added_noise = np.random.rand() *  np.random.randint(lower_noise_limit, upper_noise_limit)\n",
    "        y.append(i + added_noise)\n",
    "    return x, y\n",
    "\n",
    "def show_best_fit(x, y):\n",
    "    '''Plots a line of best fit on top of the data points'''\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color='red', linewidth=5)\n",
    "    m, b  = np.polyfit(x, y, 1)\n",
    "    plt.title(\"f(x) = {}x + {}\".format(m, b))\n",
    "    \n",
    "x, y = generate_dataset()\n",
    "show_best_fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Using Mulitple Regression for the Boston Housing Prices Dataset</h2></center>\n",
    "\n",
    "In the example above, we were able to graph our findings because we only used 1 predictor.  Although we can't graph really graph a regression that uses more than 2 predictors, we can still fit the regression and use it for making predictions.  In this problem set, that's exactly what we'll do.\n",
    "\n",
    "<center><h3>About the Data Set</h3></center>\n",
    "\n",
    "The `Boston_Housing_Prices` dataset is a famous dataset usually used for learning or benchmarking regression techniques.  This dataset contains 14 variables, one of which (MEDV, median value of home) we'll use as our target. All other variables are continuous, with the exception of 1 being binary (if the house shares a border with the Charles River).  For a full description of each predictor, you can [follow this link](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names), or just examine the `.DESC` attribute of the object `load_boston()` returns.  \n",
    "\n",
    "<center><h3>Your Task</h3></center>\n",
    "\n",
    "For this assignment, your task is to:\n",
    "\n",
    "1. Read in the dataset using sklearn's `load_boston()` function (found in `sklearn.datasets`)\n",
    "<br>\n",
    "<br>\n",
    "2. Perform some basic exploratory data analysis to get a feel for the data.  Graph some stuff!\n",
    "<br>\n",
    "<br>\n",
    "3. Create a **_correlation heatmap_** to check to see how highly correlated our predictor variables are (Remember, if our predictors are highly correlated, this is bad.) . \n",
    "<br>\n",
    "<br>\n",
    "4. Split the data into training and testing, or use K-Fold Cross Validation.  Your choice.  \n",
    "<br>\n",
    "5. Fit a regression model.  Examine the results.  If any of the predictors have a P-value of greater than 0.05, remove that predictor from your dataset and rerun the regression.  Repeat until you have a well-fit regression model. \n",
    "<br>\n",
    "<br>\n",
    "6. Make predictions on your test set (X_test) and see how well it compares to the actual targets (y_test) from the test set.  \n",
    "\n",
    "\n",
    "<center>**_Step 1: Import the Data_**</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the load_boston() function from sklearn.datasets\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = None\n",
    "\n",
    "# Create a dataframe object out of the data.  Use boston.data for the data, and pass boston.feature_names \n",
    "# for the columns parameter\n",
    "df = None\n",
    "\n",
    "# Store the data in boston.target in the targets variable\n",
    "targets = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**_Step 2: Exploratory Data Analysis_**</center>\n",
    "\n",
    "Be sure to familiarize yourself with this dataset enough to answer the following questions:\n",
    "\n",
    "1.  Are there any null values in this dataset?\n",
    "2.  Are there any outliers in this dataset?\n",
    "3.  What are the descriptive statistics of each predictor?\n",
    "\n",
    "If you run across any null values or outliers, deal with them, and make note of your strategy for doing so. \n",
    "\n",
    "**_A Note on the Data in this Data Set:_**: If you examine the predictors, you'll likely notice one of the predictors, B, is problematic for a number of reasons.  This predictor is a measure of how much of the neighborhood was occupied by Black residents at the time.  This is not a majorly predictive attribute, but even if it was, it it brings up questions about whether we _should_ use data on things such as race, gender, etc in our models.  We have chosen to leave this attribute in the dataset because this is what the actual dataset contains, but with the caveat that we will be using this to have a larger discussion about data ethics in class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Exploratory Data Analysis here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Step 3: Create a Correlation Heatmap_**\n",
    "\n",
    "In this step, we're going to use the `seaborn` package to crete a correlation heatmap, so that we can see how much correlation there is between each predictor variable.  \n",
    "\n",
    "The code for building a heatmap can seem a bit intimidating, but don't worry--it's nothing you need to memorize.  You'll find great instructions and examples [in this tutorial](http://www.neural.cz/dataset-exploration-boston-house-pricing.html).  You can also look at the [seaborn docs for heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html).  \n",
    "\n",
    "**Note:** The tutorial uses an older deprecated method that still works, and is likely the easier route here.  In the current version of `seaborn`, you'll find it inside `sns.linearmodels`.  \n",
    "\n",
    "Your heatmap should look something like this when completed:\n",
    "\n",
    "<center><img src='img/heatmap.png' height=60% width=60%></center>\n",
    "\n",
    "\n",
    "<center><h3>How to Interpret a Correlation Heatmap</h3></center>\n",
    "\n",
    "This graph can look a little overwhelming at first, but it's not as confusing as it seems.  The whole idea of a correlation heatmap is to line up the columns of the dataset into a grid, and make it easily readable to see what correlates with what.  Each combination of columns is represented as a unique cell in the grid.  \n",
    "\n",
    "You might notice that the diagonal through the center of the grid is our key.  This makes sense, because if we plotted each at the same position on the x-axis and y-axis (for instance, CRIM would be the first column and the first row), then the box that currently says CRIM would be where we see a measure of CRIM's correlation with itself.  Since any variable's correlation with itself will always be 1, this is a nonsense column, this makes it a great place to put the variable names.  \n",
    "\n",
    "The best way to read a correlation heatmap is to find the variable you're looking for, and realize that the both row and column extending from it all correspond to that variable.  Anything below the diagonal is going to be mirrored across the diagonal as well.  In the example above, 2 cells down from `INDUS` and 2 cells right of `INDUS` are the exact same thing--the correlation between `INDUS` and `NOX`.  Cells below the diagonal represent this information through color, whereas the cells above the diagonal just give us the actual value.  \n",
    "\n",
    "Create the heatmap, and then use it to determine which variables have the highest correlation with other variables.  This will be important to help you figure out which columns to remove when tuning your regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a heatmap using sns.linearmodels.corrplot(), or sns.heatmap().  If you use .heatmap(), you'll need to \n",
    "# create a correlation matrix by using np.corrcoef() on your data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Steps 4 - 6: Creating and Tuning your Regression</h3></center>\n",
    "\n",
    "For the remainder of the notebook, you'll be creating, fitting, and tweaking your Linear Regression.  \n",
    "\n",
    "**_TASKS_**:\n",
    "\n",
    "1. Split your data into training and testing sets using train_test_split (alternatively, you can opt to use K-Fold Cross Validation. Either is available in `sklearn`). \n",
    "<br>\n",
    "<br>\n",
    "2. Use a [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object from SKlearn, and fit it on your training data.  If you run into problems, use the docs and/or Stack Overflow to help!\n",
    "<br>\n",
    "<br>\n",
    "3. Use it to predict for the data in `X_test`, and plot them against the actual values contained in `y_test`.\n",
    "<br>\n",
    "<br>\n",
    "4. Examine your R<sup>2</sup> value.  How much of the variance does your model explain?\n",
    "<br>\n",
    "<br>\n",
    "5. Optional: Tweak your model.  See if you can increase your R<sup>2</sup> by dealing with highly correlated predictors, normalizing your continuous data, etc.  Try whatever data science techniques you've learned so far (or that you can discover on the internet) to make this model more accurate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MS-ML-P3]",
   "language": "python",
   "name": "conda-env-MS-ML-P3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
